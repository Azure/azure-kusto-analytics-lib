{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Train a room occupancy prediction model with Azure Machine Learning and score with ADX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open dataset from UCI Repository: __[Occupancy Detection](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+)__\n",
    "\n",
    "Experimental data used for binary classification (room occupancy) from Temperature,Humidity,Light and CO2.\n",
    "Ground-truth occupancy was obtained from time stamped pictures that were taken every minute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "* Enable Python plugin on your ADX cluster (see the Onboarding section of the __[python() plugin doc](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin?pivots=azuredataexplorer)__)\n",
    "* Whitelist a blob container to be accessible by ADX Python sandbox (see the Appendix section of the doc)\n",
    "* Create a Python environment (conda or virtual env) that reflects the Python sandbox image\n",
    "* Install in that environment AML SDK\n",
    "* Install in that environment Azure Blob Storage SDK (intall the older version v2.1 as the newer version is currently incompatible with azure-kusto-ingest package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your AML environment\n",
    "\n",
    "* Import Python packages\n",
    "* Create (or connect to) an AML workspace\n",
    "* Create (or connect to) a remote compute target to use for training\n",
    "* Create an experiment to track all your runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing AML packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "print(sys.version)\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create workspace\n",
    "\n",
    "If the workspace already exists connect to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.create(\n",
    "    name = \"Your Workspace Name\",\n",
    "    subscription_id = \"Your Subsription Id\",\n",
    "    resource_group = \"Your Resource Group\", \n",
    "    location = \"Your location\",  # e.g \"westus\"\n",
    "    exist_ok = True,\n",
    "    show_output = True)\n",
    "\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# Just for testing: load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name=\"Prediction-Occupancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines.Here you create Azure Machine Learning Compute for model training\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [],
   "source": [
    "compute_name = \"cpu-cluster\"\n",
    "vm_sku = \"STANDARD_D2_V2\"\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_sku, min_nodes=1,max_nodes=2)\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "* Fetch the occupancy detection dataset from Kusto using __[KqlMagic](https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic)__\n",
    "\n",
    "* Display some records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_ext Kqlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%kql kusto://code;cluster='demo11.westus';database='ML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%kql res << OccupancyDetection\n",
    "df = res.to_dataframe() \n",
    "print(df.shape)\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's copy the data from ADX to blob container to access it from AML\n",
    "\n",
    "Notes:\n",
    "1. We copy the input data using KqlMagic to a blob container in the storage account that was allocated for the AML workspace\n",
    "2. You can create the  blob container using __[Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/)__, and extract its SAS token by right clicking it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_storage_account = \"Your storage account\" # you can use the storage account that was created automatically as part of the AML workspace\n",
    "aml_container_name = \"kusto\"\n",
    "aml_sas_token = \"Your SAS Token for this container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blob_container_uri = f\"https://{aml_storage_account}.blob.core.windows.net/{aml_container_name}{aml_sas_token}\"\n",
    "copy_query = f\".export to csv (h@'{blob_container_uri}') with(includeHeaders=all) <| OccupancyDetection\"\n",
    "print(copy_query)\n",
    "\n",
    "%kql res << -query copy_query\n",
    "data_blob_name = res.to_dataframe()[\"Path\"][0].split('/')[-1]\n",
    "print(\"\\ndata blob name is: \", data_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test downloading the blob\n",
    "\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlockBlobService  # v2.1\n",
    "block_blob_service = BlockBlobService(account_name=aml_storage_account, sas_token=aml_sas_token)\n",
    "block_blob_service.get_blob_to_path(aml_container_name, data_blob_name, 'data.csv')\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a remote cluster\n",
    "\n",
    "Here we submit the job to run on the remote training cluster we set up earlier. To submit a job we:\n",
    "* Create a directory for all files to be uploaded to the remote cluster\n",
    "* Create a training script\n",
    "* Create an estimator object\n",
    "* Submit the job \n",
    "\n",
    "### Create a directory\n",
    "\n",
    "Create a directory to upload all files to the remote cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"to-upload\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, we need to create a training script. Here we create `train.py` in the `to-upload` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$script_folder/train.py\"\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlockBlobService  # v2.1\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--account', type=str, dest='storage_account', help='storage account name')\n",
    "parser.add_argument('--container', type=str, dest='container_name', help='blob container name')\n",
    "parser.add_argument('--blob', type=str, dest='blob_name', help='blob name')\n",
    "parser.add_argument('--sas', type=str, dest='sas_token', help='SAS token')\n",
    "args = parser.parse_args()\n",
    "\n",
    "storage_account = args.storage_account\n",
    "container_name = args.container_name\n",
    "blob_name = args.blob_name\n",
    "sas_token = args.sas_token\n",
    "\n",
    "# downloading the blob to a local file 'data.csv' and read into a dataframe\n",
    "\n",
    "block_blob_service = BlockBlobService(account_name=storage_account, sas_token=sas_token)\n",
    "block_blob_service.get_blob_to_path(container_name, blob_name, 'data.csv')\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "train_x = df[df['Test'] == False][['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']]\n",
    "train_y = df[df['Test'] == False]['Occupancy']\n",
    "test_x = df[df['Test'] == True][['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']]\n",
    "test_y = df[df['Test'] == True]['Occupancy']\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "#four classifier types\n",
    "clf1 = tree.DecisionTreeClassifier()\n",
    "clf2 = LogisticRegression(solver='liblinear')\n",
    "clf3 = neighbors.KNeighborsClassifier()\n",
    "clf4 = naive_bayes.GaussianNB()\n",
    "\n",
    "clf1 = clf1.fit(train_x, train_y)\n",
    "clf2 = clf2.fit(train_x, train_y)\n",
    "clf3 = clf3.fit(train_x, train_y)\n",
    "clf4 = clf4.fit(train_x, train_y)\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True) # note files saved in the outputs folder are automatically uploaded into experiment\n",
    "\n",
    "# Accuracy on training set\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4], ['Decision Tree', 'Logistic Regression', 'K Nearest Neighbour', 'Naive Bayes']):\n",
    "            scores = cross_val_score(clf, train_x, train_y, cv=5, scoring='accuracy')\n",
    "            print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "            run.log(\"training accuracy\", scores.mean(), description='accuracy over the training set')\n",
    "            \n",
    "# Accuracy on testing set\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4], ['Decision Tree', 'Logistic Regression', 'K Nearest Neighbour', 'Naive Bayes']):\n",
    "            scores = cross_val_score(clf, test_x, test_y, cv=5, scoring='accuracy')\n",
    "            print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "            run.log(\"model type\", label)\n",
    "            run.log(\"testing accuracy\", scores.mean(), description='accuracy over the testing set')\n",
    "            with open('outputs/' + label+'.pkl', 'wb') as handle:\n",
    "                pickle.dump(clf, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator\n",
    "\n",
    "An estimator object is used to submit the run. Azure Machine Learning has pre-configured estimators for common machine learning frameworks, as well as generic Estimator. We create SKLearn estimator for scikit-learn model, by specifying\n",
    "\n",
    "* The name of the estimator object, `est`\n",
    "* The directory to uploaded into the cluster nodes for execution. \n",
    "* The compute target that we created\n",
    "* The training script name `train.py`\n",
    "* Parameters required from the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Environment to install required packages\n",
    "\n",
    "env = Environment('adx_sandbox_env')\n",
    "# set scikit-learn==0.20.3 to match Kusto Python sandbox image (as of 4/2020)\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk','scikit-learn==0.20.3','pandas==0.24.1','azure.storage.blob==2.1.0','azureml-dataprep[pandas,fuse]>=1.1.14'])\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "script_params = {\n",
    "    '--account': aml_storage_account,\n",
    "    '--container': aml_container_name,\n",
    "    '--blob': data_blob_name,\n",
    "    '--sas': aml_sas_token\n",
    "}\n",
    "\n",
    "# Create the Estimator\n",
    "\n",
    "est = SKLearn(source_directory=script_folder,\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              environment_definition=env,\n",
    "              entry_script='train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "We run the experiment by submitting the estimator object; we can navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies (`conda_packages` parameter in the above estimator constructor) don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. We can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied (not relevant in this example as we read the data from blob), then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. We can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so we can access these results.\n",
    "\n",
    "\n",
    "We can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training happens in the background. We can use `wait_for_completion` to block and wait until the model has completed training before running more code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "# specify show_output to True for a verbose log\n",
    "run.wait_for_completion(show_output=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "We now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get metrics"
    ]
   },
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "The training script pickled the models to files and wrote them in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to our workspace.  This content appears in the run record in the experiment under the workspace. Hence, the model file is now also available in the workspace.\n",
    "\n",
    "We can see files associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history"
    ]
   },
   "outputs": [],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that we can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from history"
    ]
   },
   "outputs": [],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='LogisticRegression', model_path='outputs/Logistic Regression.pkl')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring in ADX\n",
    "\n",
    "2 options for retrieving the model for scoring:\n",
    "- serialize the model to a string to be stored in a standard table in ADX\n",
    "- copy the model to a blob container (that was previously whitelisted for access by ADX Python sandbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model.download(exist_ok=True)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring from serialized model which is stored in ADX table\n",
    "\n",
    "Serializing the model and store it in ADX models table using KqlMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "models_tbl = 'ML_Models'\n",
    "model_name = 'AML-Occupancy'\n",
    "\n",
    "with open(model_path, 'rb') as handle:\n",
    "    buf = handle.read()\n",
    "\n",
    "smodel = buf.hex()\n",
    "now = datetime.datetime.now()\n",
    "dfm = pd.DataFrame({'name':[model_name], 'timestamp':[now], 'model':[smodel]})\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_query = '''\n",
    ".set-or-append {0} <|\n",
    "let tbl = dfm;\n",
    "tbl\n",
    "'''.format(models_tbl)\n",
    "print(set_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%kql -query set_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring from serialized model which is stored in ADX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we run ADX scoring query here using KqlMagic by embedding the query from Kusto Explorer\n",
    "# with r'''Kusto Explorer query''':\n",
    "\n",
    "scoring_from_table_query = r'''\n",
    "let classify_sf=(samples:(*), models_tbl:(name:string, timestamp:datetime, model:string), model_name:string, features_cols:dynamic, pred_col:string)\n",
    "{\n",
    "    let model_str = toscalar(models_tbl | where name == model_name | top 1 by timestamp desc | project model);\n",
    "    let kwargs = pack('smodel', model_str, 'features_cols', features_cols, 'pred_col', pred_col);\n",
    "    let code =\n",
    "    'import pickle\\n'\n",
    "    'import binascii\\n'\n",
    "    '\\n'\n",
    "    'smodel = kargs[\"smodel\"]\\n'\n",
    "    'features_cols = kargs[\"features_cols\"]\\n'\n",
    "    'pred_col = kargs[\"pred_col\"]\\n'\n",
    "    'bmodel = binascii.unhexlify(smodel)\\n'\n",
    "    'clf1 = pickle.loads(bmodel)\\n'\n",
    "    'df1 = df[features_cols]\\n'\n",
    "    'predictions = clf1.predict(df1)\\n'\n",
    "    '\\n'\n",
    "    'result = df\\n'\n",
    "    'result[pred_col] = pd.DataFrame(predictions, columns=[pred_col])'\n",
    "    '\\n'\n",
    "    ;\n",
    "    samples | evaluate python(typeof(*), code, kwargs)\n",
    "};\n",
    "OccupancyDetection \n",
    "| where Test == 1\n",
    "| extend pred_Occupancy=bool(0)\n",
    "| invoke classify_sf(ML_Models, 'AML-Occupancy', pack_array('Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio'), 'pred_Occupancy')\n",
    "| summarize n=count() by Occupancy, pred_Occupancy      //  confusion matrix\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%kql res << -query scoring_from_table_query\n",
    "df = res.to_dataframe()\n",
    "print('Confusion Matrix')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring from model which is stored in blob storage\n",
    "\n",
    "Copy the model to blob\n",
    "\n",
    "Note again that the blob container should be whitelisted to be accessible by ADX Python sandbox (see the appendix section of the __[python() plugin doc](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin?pivots=azuredataexplorer)__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adx_storage_account = \"Your Storage Account\"\n",
    "adx_container_name = \"Your container\"\n",
    "model_blob_name = model_name + '.pkl'\n",
    "adx_sas_token = \"Your SAS Token for this container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService  # v2.1\n",
    "block_blob_service = BlockBlobService(account_name=adx_storage_account, sas_token=adx_sas_token)\n",
    "block_blob_service.create_blob_from_path(adx_container_name, model_blob_name, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f'https://{adx_storage_account}.blob.core.windows.net/{adx_container_name}/{model_blob_name}{adx_sas_token}'\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_from_blob_query = r'''\n",
    "let classify_sf=(samples:(*), model_sas:string, features_cols:dynamic, pred_col:string)\n",
    "{\n",
    "    let kwargs = pack('model_sas', model_sas, 'features_cols', features_cols, 'pred_col', pred_col);\n",
    "    let code =\n",
    "    '\\n'\n",
    "    'import pickle\\n'\n",
    "    '\\n'\n",
    "    'model_sas = kargs[\"model_sas\"]\\n'\n",
    "    'features_cols = kargs[\"features_cols\"]\\n'\n",
    "    'pred_col = kargs[\"pred_col\"]\\n'\n",
    "    'with open(\"/Temp/model.pkl\", \"rb\") as f:\\n'\n",
    "    '   bmodel = f.read()\\n'\n",
    "    'clf1 = pickle.loads(bmodel)\\n'\n",
    "    'df1 = df[features_cols]\\n'\n",
    "    'predictions = clf1.predict(df1)\\n'\n",
    "    '\\n'\n",
    "    'result = df\\n'\n",
    "    'result[pred_col] = pd.DataFrame(predictions, columns=[pred_col])'\n",
    "    '\\n'\n",
    "    ;\n",
    "    samples | evaluate python(typeof(*), code, kwargs,\n",
    "        external_artifacts=pack('model.pkl', model_sas))\n",
    "};\n",
    "OccupancyDetection \n",
    "| where Test == 1\n",
    "| extend pred_Occupancy=bool(0)\n",
    "| invoke classify_sf('$model_uri$',\n",
    "                     pack_array('Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio'), 'pred_Occupancy')\n",
    "| summarize n=count() by Occupancy, pred_Occupancy      //  confusion matrix\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_from_blob_query = scoring_from_blob_query.replace('$model_uri$', model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%kql res << -query scoring_from_blob_query\n",
    "df = res.to_dataframe()\n",
    "print('Confusion Matrix')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this tutorial you learned how to train a model in AML and then use ADX for scoring. This is a win-win scenario as:\n",
    "* AML has the infrastructue for batch training that can be done on scalable compute nodes of misc. SKUs\n",
    "* AML enables ML Ops - full management of ML workflow (including the training data, ML model selection, hyper parameters tuning etc.)\n",
    "* ADX scoring is done near the data, on the same ADX compute nodes, enabling near real time processing of big amounts of new data. There is no the need to export the data to external scoring service and import back the results. Consequently, scoring architecture is simpler and performance is much faster and scalable"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "kernelspec": {
   "display_name": "Python - Kusto sandbox with AML",
   "language": "python",
   "name": "sandbox-aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "msauthor": "roastala"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
